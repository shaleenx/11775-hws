{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyframes(downsampled_video_filename, keyframe_interval): \n",
    "    \"Generator function which returns the next keyframe.\"         \n",
    "\n",
    "    # Create video capture object                                     \n",
    "    video_cap = cv2.VideoCapture(downsampled_video_filename)                                     \n",
    "    frame = 0                                                                                    \n",
    "    while True:                                                                                  \n",
    "        frame += 1                                                                               \n",
    "        ret, img = video_cap.read()                                                              \n",
    "        if ret is False:                                                                         \n",
    "            break                                                                                \n",
    "        if frame % keyframe_interval == 0:\n",
    "              yield img\n",
    "    video_cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_names = \"tryvideos\"\n",
    "config_file = \"config.yaml\"                                                                  \n",
    "my_params = yaml.load(open(config_file))                                                   \n",
    "                                                                                           \n",
    "# Get parameters from config file                                                          \n",
    "keyframe_interval = my_params.get('keyframe_interval')                                                                          \n",
    "cnn_features_folderpath = my_params.get('cnn_features')                                  \n",
    "downsampled_videos = my_params.get('downsampled_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator:  0.0002124309539794922\n"
     ]
    }
   ],
   "source": [
    "# fread = open(all_video_names, \"r\")\n",
    "# for line in fread.readlines():\n",
    "line = \"HVC1012\"\n",
    "\n",
    "start = time.time()\n",
    "video_name = line.replace('\\n', '')\n",
    "downsampled_video_filename = os.path.join(downsampled_videos, video_name + '.ds.mp4')\n",
    "cnn_feat_video_filename = os.path.join(cnn_features_folderpath, video_name + '.cnnn')\n",
    "\n",
    "if (os.path.isfile(cnn_feat_video_filename)) or (not os.path.isfile(downsampled_video_filename)):\n",
    "    print(\"skipping\", video_name)\n",
    "#     continue\n",
    "start = time.time()\n",
    "keyframe_iterator = get_keyframes(downsampled_video_filename, keyframe_interval)\n",
    "print(\"iterator: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "\n",
    "layer = resnet_model._modules.get('avgpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.eval()\n",
    "model = resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = transforms.Scale((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name):\n",
    "    # 1. Load the image with Pillow library\n",
    "    img = Image.fromarray(image_name)\n",
    "    # 2. Create a PyTorch Variable with the transformed image\n",
    "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "    # 3. Create a vector of zeros that will hold our feature vector\n",
    "    #    The 'avgpool' layer has an output size of 512\n",
    "    my_embedding = torch.zeros(512)\n",
    "    # 4. Defining a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data.squeeze())\n",
    "    # 5. Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "    # 6. Run the model on our transformed image\n",
    "    resnet_model(t_img)\n",
    "    # 7. Detach our copy function from the layer\n",
    "    h.remove()\n",
    "    # 8. Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(img).show()\n",
    "\n",
    "feat = np.vstack([get_vector(img) for img in keyframe_iterator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 512)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unused Code\n",
    "\n",
    "# model = models.alexnet(pretrained=True)\n",
    "\n",
    "# # remove last fully-connected layer\n",
    "# # new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "# # model.classifier = new_classifier\n",
    "\n",
    "\n",
    "# original_model = model\n",
    "\n",
    "# class AlexNetConv4(nn.Module):\n",
    "#             def __init__(self):\n",
    "#                 super(AlexNetConv4, self).__init__()\n",
    "#                 self.features = nn.Sequential(\n",
    "#                     # stop at conv4\n",
    "#                     *list(original_model.features.children())[:-1]\n",
    "#                 )\n",
    "#             def forward(self, x):\n",
    "#                 x = self.features(x)\n",
    "#                 return x\n",
    "\n",
    "# model = AlexNetConv4()\n",
    "\n",
    "# # Data augmentation and normalization for training\n",
    "# # Just normalization for validation\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "\n",
    "# mytransform = transforms.Compose([\n",
    "# transforms.Resize(256),\n",
    "# transforms.CenterCrop(224),\n",
    "# transforms.ToTensor(),\n",
    "# transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval()\n",
    "# # pil_image = Image.fromstring(\"L\", cv2.GetSize(img), img.tostring())\n",
    "# pil_image = Image.fromarray(img)\n",
    "# transformed_image = mytransform(pil_image).unsqueeze_(0)\n",
    "\n",
    "\n",
    "# # model(torch.Tensor(mytransform(img)))\n",
    "\n",
    "# feat = model(Variable(transformed_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
